{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9c965173",
   "metadata": {},
   "outputs": [],
   "source": [
    "#run script to train with kindergarten cl\n",
    "\n",
    "from dynamics.process.rnn import fulltrain\n",
    "\n",
    "#location of repo\n",
    "workdir='~/projects/kind_cl/'\n",
    "\n",
    "#location of config file\n",
    "cfgname='/Users/dhocker/projects/kind_cl/demo/33.cfg'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "11e5109a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ops\n",
      "{'gamma': 0.95, 'alpha': 0.0001, 'lam': 2.5, 'pcatch': 0.2, 'ctmax': 500, 'blocklength': 40, 'freeze': False, 'state_resetfreq': 160, 'p_statereset': 1.0, 'prog_stop': False, 'envtype': 'wt_env_wITI_batchedreward', 'iti_determ': 1, 'iti_random': 20, 'iti_type': 'uniform', 'pblock': 0.5, 'trainonly': True, 'tracktheta': False, 'trackstate': False, 'plot': False, 'device': 'cpu', 'Vvec': [5, 10, 20, 40, 80], 'Vvec_low': [5, 10, 20], 'Vvec_high': [20, 40, 80], 'Ract': -2.0, 'w': -0.05, 'win': 20, 'R_vio': -1.0, 'seed': 101, 'ns': 10000, 'T': 30, 'dt': 0.05, 'blocklen': 40, 'useblocks': True, 'displevel': 4, 'inputcase': 11, 'batchsize': 1, 'snr': [0.0, 0.0, 0.0, 0.0], 'snr_hidden': 0.0, 'p_vio': 0.0, 'T_vio': 0.1, 'rnntype': 'LSTM', 'dropout': 0.0, 'modeltype': 'RNNModel_multiregion_2allt', 'costtype': 'loss_actorcritic_regularized_9', 'ismultiregion': True, 'rank': [None, None], 'kindergarten_prediction': True, 'nocatch_force': False, 'alpha_rho': 0.3, 'alpha_rho_av': 0.05, 'lambda_policy': 1.0, 'lambda_value': 0.0005, 'lambda_entropy': 0.05, 'lambda_pred': 0.5, 'anneal_type': 'none', 'gamma_kindergarten': 0.0001, 'lambda_supervised': 10.0, 'seed_kindergarten': 0, 'lossinds_kindergarten': [0, 1, 2], 'batchsize_kindergarten': 1000, 'stages_kindergarten': ['simple', 'intermediate'], 'nepoch_kindergarten': 10000, 'stopargs_int_kindergarten': 0.001, 'base_epoch_kind': 1000, 'nsteps_simple_kindergarten': 20, 'nsteps_train_simple_kindergarten': 20, 'nsteps_list_kindergarten': [20, 25], 'nsteps_list_int': array([31, 90, 83, 95, 29, 97, 60, 24, 83, 60]), 'highvartrials_kind': False, 'nsteps_train_int': 652, 'stoparg_simple': 30.0, 'stoptype_simple': 'converge', 'pred_stoparg': 0.2, 'pred_stoptype': 'lowlim', 'beta_pred_kindpred': 0.5, 'beta_supervised_kindpred': 1.0, 'lossinds_supervised_kindpred': [0, 1, 2], 'batchsize_pred': 20, 'ntrials_pred': 400, 'gamma_pred': 0.005, 'nepoch_pred': 10000, 'base_epoch_pred': 20, 'pred_updatefun': 'update_trialstart', 'kindergarten_d2m': False, 'd2m_p': 0.5, 'lambda_d2m': 0.0, 'beta_d2m_kindd2m': 1.0, 'base_epoch_d2m': 100, 'gamma_d2m': 0.001, 'beta_pred_d2m': 0.0, 'beta_supervised_kindd2m': 0.0, 'batchsize_d2m': 1000, 'tmin_d2m': 0.1, 'tmax_d2m': 5, 'nepoch_d2m': 10000, 'd2m_updatefun': 'update_both', 'lossinds_supervised_kindd2m': [1, 2], 'd2m_stoptype': 'lowlim', 'd2m_stoparg': 0.01, 'pstim_prop': 0.4, 'pstim_amp': 1.0, 'nn': [256, 256]}\n",
      "uniqueops\n",
      "{'netseed': 33, 'device': 'cpu', 'savedir': '/Users/dhocker/projects/kind_cl/results/', 'modelname': '', 'nn': [256, 256], 'bias': 0.0, 'nrounds1': 10, 'nrounds2': 10, 'nrounds3': 10, 'nrounds1_s': 0, 'nrounds2_s': 0, 'nrounds3_s': 0, 'usekindergarten': True, 'roundstart_pred': 0, 'adam_fname': ''}\n",
      "choosing model type\n",
      "moving model to device\n",
      "saving initial network\n",
      "deciding on pre-processing steps\n",
      "beginning kindergarten\n",
      "/Users/dhocker/projects/kind_cl/results/rnn_kindergarten_33\n",
      "training on simple task\n",
      "task inds for training:\n",
      "[[0], [1, 0], [2, 1, 0]]\n",
      "training on output dims:[0]\n",
      "9.407548\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m fulltrain\u001b[38;5;241m.\u001b[39mfulltrain_run(cfgname)\n",
      "File \u001b[0;32m~/projects/dynamics/dynamics/process/rnn/fulltrain.py:59\u001b[0m, in \u001b[0;36mfulltrain_run\u001b[0;34m(fname)\u001b[0m\n\u001b[1;32m     57\u001b[0m     savename \u001b[38;5;241m=\u001b[39m savedir \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrnn_kindergarten_\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(netseed)\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;28mprint\u001b[39m(savename)\n\u001b[0;32m---> 59\u001b[0m     net, _, _, optim_fname \u001b[38;5;241m=\u001b[39m wt_protocols\u001b[38;5;241m.\u001b[39mtraining_kindergarten(net, ops\u001b[38;5;241m=\u001b[39mops, savename\u001b[38;5;241m=\u001b[39msavename, device\u001b[38;5;241m=\u001b[39mdevice,\n\u001b[1;32m     60\u001b[0m                                                                 savelog\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, optim_fname\u001b[38;5;241m=\u001b[39moptim_fname)\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ops[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkindergarten_prediction\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkindergarten + prediction\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/projects/dynamics/dynamics/process/rnn/wt_protocols.py:152\u001b[0m, in \u001b[0;36mtraining_kindergarten\u001b[0;34m(net, ops, savename, device, savelog, optim_fname)\u001b[0m\n\u001b[1;32m    149\u001b[0m     si\u001b[38;5;241m.\u001b[39mdetach()\n\u001b[1;32m    151\u001b[0m \u001b[38;5;66;03m# train, round 1\u001b[39;00m\n\u001b[0;32m--> 152\u001b[0m ltot_train, grad_norm, outptdict \u001b[38;5;241m=\u001b[39m wt_kindergarten\u001b[38;5;241m.\u001b[39mtrain(net, updater, si, inputs, targets, device\u001b[38;5;241m=\u001b[39mdevice,\n\u001b[1;32m    153\u001b[0m                                                          nepoch\u001b[38;5;241m=\u001b[39mnepoch, nsteps\u001b[38;5;241m=\u001b[39mnsteps_train_simple,\n\u001b[1;32m    154\u001b[0m                                                          lossinds\u001b[38;5;241m=\u001b[39mlossinds, stoptype\u001b[38;5;241m=\u001b[39mstoptype_simple,\n\u001b[1;32m    155\u001b[0m                                                          stopargs\u001b[38;5;241m=\u001b[39mstoparg_simple)\n\u001b[1;32m    157\u001b[0m ltot_train_list_simple\u001b[38;5;241m.\u001b[39mappend(ltot_train)\n\u001b[1;32m    158\u001b[0m gradnorm_list_simple\u001b[38;5;241m.\u001b[39mappend(grad_norm)\n",
      "File \u001b[0;32m~/projects/dynamics/dynamics/process/rnn/wt_kindergarten.py:271\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(net, updater, si, inp, targ, device, nepoch, nsteps, lossinds, stoptype, stopargs, win, chkptepoch)\u001b[0m\n\u001b[1;32m    268\u001b[0m outputs, state, net \u001b[38;5;241m=\u001b[39m batchsamples(net, inputs_k, state, device)\n\u001b[1;32m    270\u001b[0m \u001b[38;5;66;03m# update\u001b[39;00m\n\u001b[0;32m--> 271\u001b[0m loss_k \u001b[38;5;241m=\u001b[39m update(updater, outputs, targets_k, lossinds)\n\u001b[1;32m    272\u001b[0m ltot_train_k\u001b[38;5;241m.\u001b[39mappend(loss_k)\n\u001b[1;32m    274\u001b[0m \u001b[38;5;66;03m# track gradient\u001b[39;00m\n",
      "File \u001b[0;32m~/projects/dynamics/dynamics/process/rnn/wt_kindergarten.py:210\u001b[0m, in \u001b[0;36mupdate\u001b[0;34m(updater, outputs, targets, lossinds)\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m updater \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:  \u001b[38;5;66;03m# just use to calculate loss, like for mixed loss funs in other trainings\u001b[39;00m\n\u001b[1;32m    209\u001b[0m     updater\u001b[38;5;241m.\u001b[39mzero_grad()  \u001b[38;5;66;03m# updaters accumulate gradients. zero out\u001b[39;00m\n\u001b[0;32m--> 210\u001b[0m     lval\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m    211\u001b[0m     updater\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    212\u001b[0m     loss_np \u001b[38;5;241m=\u001b[39m lval\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()  \u001b[38;5;66;03m# the value of the loss from that batch\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/torchenv/lib/python3.11/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[1;32m    489\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/torchenv/lib/python3.11/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    202\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "fulltrain.fulltrain_run(cfgname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8256c9ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchenv",
   "language": "python",
   "name": "torchenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
