{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "5eb3fbaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "#Second part of figure 3. the weight changes\n",
    "\n",
    "import torch\n",
    "import sys\n",
    "import os\n",
    "import io\n",
    "from os.path import exists\n",
    "import numpy as np\n",
    "import pickle\n",
    "from scipy.special import softmax\n",
    "import json\n",
    "from scipy import stats\n",
    "import gzip, pickle, pickletools\n",
    "import warnings\n",
    "import glob\n",
    "import copy\n",
    "\n",
    "from scipy.signal import butter,filtfilt\n",
    "\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from scipy.stats import ranksums, wilcoxon\n",
    "import statsmodels.api as sm\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "matplotlib.rcParams['pdf.fonttype'] = 42\n",
    "matplotlib.rcParams['ps.fonttype'] = 42\n",
    "from scipy.stats import ranksums, wilcoxon\n",
    "\n",
    "from dynamics.utils import utils\n",
    "from dynamics.vis import dynamics as dyplot\n",
    "\n",
    "\n",
    "from dynamics.process.rnn import wt_kindergarten, wt_nets, wt_costs, wt_reinforce_cont_new, wt_pred, parse\n",
    "from dynamics.vis import wt_vis\n",
    "from dynamics.analysis import wt_analysis as wta\n",
    "from dynamics.utils.utils import CPU_Unpickler,parse_configs, displ, memcheck, opsbase, mwa\n",
    "from dynamics.analysis import state_analysis as sta\n",
    "from dynamics.analysis import optim\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from scipy import interpolate\n",
    "import statsmodels.api as sm\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "#%matplotlib notebook\n",
    "#%matplotlib widget \n",
    "#out = widgets.Output(layout = widgets.Layout(height='300px'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16db4491",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#path info\n",
    "\n",
    "\n",
    "kuse = [10,60,40]  # the maximum index for the final stage 5 and stage 6 training stage, by s_idx\n",
    "\n",
    "\n",
    "\n",
    "dbase = '/scratch/dh148/dynamics/results/rnn/ac/20231003/'\n",
    "# for figure 3\n",
    "subdirlist = ['full_cl/', 'nok_cl/','nok_nocl/', 'full_cl_redo/', 'pkind_mem/', 'pkind_pred/']\n",
    "# for supp. fig 3\n",
    "#subdirlist = ['full_cl/', 'pkind_mem/', 'pkind_pred/', 'pkind_count/','pkind_int/']\n",
    "configdir = lambda s_idx: '/home/dh148/projects/dynamics/jobs/20231003_clstudy/rnn/'+subdirlist[s_idx]\n",
    "suff = '.json' #file ending\n",
    "    \n",
    "    \n",
    "datadir_dat = lambda s_idx: dbase+subdirlist[s_idx]\n",
    "fgen = lambda num,idx, base, sess, s_idx : datadir_dat(s_idx)+str(num)+'/'+base + str(num)+'_'+sess+'_'+str(idx)\n",
    "\n",
    "fname_funs = [lambda num,idx, s_idx: datadir_dat(s_idx)+str(num)+'/'+'rnn_kindergarten_' + str(num)+'_simple',\n",
    "              lambda num,idx, s_idx: datadir_dat(s_idx)+str(num)+'/'+'rnn_kindergarten_' + str(num)+'_int_0_'+str(idx),\n",
    "              lambda num,idx, s_idx: datadir_dat(s_idx)+str(num)+'/'+'rnn_pred_' + str(num)+'_'+str(idx),\n",
    "              lambda num,idx, s_idx: fgen(num, idx, 'rnn_curric_', 'nocatch', s_idx),\n",
    "              lambda num,idx, s_idx: fgen(num, idx, 'rnn_curric_', 'catch', s_idx),\n",
    "              lambda num,idx, s_idx: fgen(num, idx, 'rnn_curric_', 'block', s_idx),\n",
    "              lambda num,idx, s_idx: fgen(num, idx, 'rnn_curric_', 'block', s_idx)+'_freeze'\n",
    "             ]\n",
    "\n",
    "configname_fun = lambda num, s_idx: configdir(s_idx) +str(num)+'.cfg'\n",
    "fname_behdat_fun = lambda num,s_idx: datadir_dat(s_idx) + 'rnn_'+str(num)+'_allbeh.json'\n",
    "fname_behdat_fun_1k = lambda num,s_idx: datadir_dat(s_idx) + 'rnn_'+str(num)+'_allbeh_1k.json'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "558b8167",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_behdat(fname_behdat, fname):\n",
    "    \"\"\" helper code to lookup the dat in the behavioral data.\n",
    "    if fname_behda is a list, just search it (for faster lookup)\n",
    "    if a filename, load the file.\"\"\"\n",
    "    \n",
    "    fname_base = fname.split('/')[-1]\n",
    "    dat = None\n",
    "    \n",
    "    if type(fname_behdat) is list:  #easier to recylce\n",
    "        datlist = fname_behdat\n",
    "    else:\n",
    "        #datlist = pickle.load(open(fname_behdat,'rb'))\n",
    "        datlist = json.load(open(fname_behdat,'r'))\n",
    " \n",
    "    for j in datlist:\n",
    "        if j['name'] == fname_base:\n",
    "            dat = j\n",
    "            break\n",
    "            \n",
    "    return dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ebcaf0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#the weights\n",
    "#output weights across training\n",
    "\n",
    "#needops\n",
    "\n",
    "def weight_outpt_acrosstraining(s_idx = 0, num = 0,n_int = 1, n_pred = 1):\n",
    "    \"\"\"\n",
    "    gets the output projection across an entire set of training. does not include kindergarten \n",
    "    s_idx: CL type 0:full, 1:nok_cl, 2: nok_nocl, others are pkind?\n",
    "    num: RNN number\n",
    "    \"\"\"\n",
    "    \n",
    "    device = torch.device('cpu')\n",
    "    modeltype = 'RNNModel_multiregion_2allt'\n",
    "    \n",
    "    stagelist = []\n",
    "    idxlist = []\n",
    "    \n",
    "    #lin sensitivity, wt ratio, historical regression, opt out, violation\n",
    "\n",
    "    w_pred = []\n",
    "    w_policy = []\n",
    "    w_val = []\n",
    "    w_rr =[]\n",
    "    w_mem = []\n",
    "    w_time = []\n",
    "    w_str_rnn = []\n",
    "    w_ofc_rnn = []\n",
    "    w_str_inp = []\n",
    "    w_ofc_inp = []\n",
    "    \n",
    "    fname_fun_stage = lambda stage, idx: fname_funs[stage](num, idx, s_idx)\n",
    "    \n",
    "    \n",
    "    #find max int and pred in the directory\n",
    "    #nmax_int = 5000\n",
    "    #nmax_pred = 50\n",
    "    nmax_simple = len(glob.glob(datadir_dat(s_idx)+str(num)+'/rnn_kindergarten_'+str(num)+'_simple.model'))\n",
    "    nmax_int = len(glob.glob(datadir_dat(s_idx)+str(num)+'/rnn_kindergarten_'+str(num)+'_int*.model'))\n",
    "    \n",
    "    #set a maximum of 100 for this to avoid a runaway simulation\n",
    "    \n",
    "    nmax_pred = np.min([100,len(glob.glob(datadir_dat(s_idx)+str(num)+'/rnn_pred_'+str(num)+'*.model'))])\n",
    "    \n",
    "    #print([nmax_simple,nmax_int, nmax_pred])\n",
    "    \n",
    "    \n",
    "    idxranges = [range(1,nmax_simple+1), range(1,nmax_int+1, n_int), range(1,nmax_pred+1, n_pred), \n",
    "                 range(1,11), range(1,11), range(1,61)]\n",
    "    #idx of networks to pull for each portion of training\n",
    "       \n",
    "    \n",
    "    #add in theinitialc ondition\n",
    "    nd = wt_nets.netdict[modeltype]  # dictionary of models and size params\n",
    "    netfun = nd['net']  \n",
    "    netseed = num\n",
    "    net = netfun(din=nd['din'], dout=nd['dout'], num_hiddens=[256,256], seed=netseed, rnntype='LSTM',\n",
    "                 bias_init=0)\n",
    "    \n",
    "    stagelist.append(-1)\n",
    "    idxlist.append(0)\n",
    "    \n",
    "    w_ofc = net.linear1.weight.detach().numpy()\n",
    "    w_str = net.linear2.weight.detach().numpy()\n",
    "    w_pred.append(w_ofc[0:3,:]) #predition\n",
    "    w_policy.append(w_str[0:3,:]) #policies\n",
    "    w_val.append(w_str[3,:])\n",
    "    w_mem.append(w_str[4,:])\n",
    "    w_time.append(w_str[5,:])\n",
    "    w_rr.append(w_ofc[3,:])\n",
    "    w_ofc_rnn.append(net.rnn1.weight_hh_l0.detach().numpy())\n",
    "    w_str_rnn.append(net.rnn2.weight_hh_l0.detach().numpy())\n",
    "    w_ofc_inp.append(net.rnn1.weight_ih_l0.detach().numpy())\n",
    "    w_str_inp.append(net.rnn2.weight_ih_l0.detach().numpy())\n",
    "    \n",
    "    wnan = np.nan*net.rnn1.weight_hh_l0.detach().numpy()\n",
    "    wnan_ofc_inp = np.nan*net.rnn1.weight_ih_l0.detach().numpy()\n",
    "    wnan_str_inp = np.nan*net.rnn2.weight_ih_l0.detach().numpy()\n",
    "    \n",
    "    \n",
    "    for stage in range(6):\n",
    "    #for stage in [4,5,6]:\n",
    "    \n",
    "        for idx in idxranges[stage]:  \n",
    "            \n",
    "            #assume what is needed is in the stats. if not, do .dat files\n",
    "            fname_model = fname_fun_stage(stage, idx)+ '.model'\n",
    "            #print(fname_model)\n",
    "\n",
    "            \n",
    "            if exists(fname_model):\n",
    "                \n",
    "                try:\n",
    "                \n",
    "                    stagelist.append(stage)\n",
    "                    idxlist.append(idx)\n",
    "\n",
    "                    #load the model\n",
    "\n",
    "                    nd = wt_nets.netdict[modeltype]  # dictionary of models and size params\n",
    "                    netfun = nd['net']  \n",
    "                    netseed = num\n",
    "                    net = netfun(din=nd['din'], dout=nd['dout'], num_hiddens=[256,256], seed=netseed, rnntype='LSTM',\n",
    "                                 bias_init=0)\n",
    "\n",
    "                    net.load_state_dict(torch.load(fname_model, map_location=device.type))\n",
    "\n",
    "                    w_ofc = net.linear1.weight.detach().numpy()\n",
    "                    w_str = net.linear2.weight.detach().numpy()\n",
    "                    w_pred.append(w_ofc[0:3,:]) #predition\n",
    "                    w_policy.append(w_str[0:3,:]) #policies\n",
    "                    w_val.append(w_str[3,:])\n",
    "                    w_mem.append(w_str[4,:])\n",
    "                    w_time.append(w_str[5,:])\n",
    "                    w_rr.append(w_ofc[3,:])\n",
    "                    w_ofc_rnn.append(net.rnn1.weight_hh_l0.detach().numpy())\n",
    "                    w_str_rnn.append(net.rnn2.weight_hh_l0.detach().numpy())\n",
    "                    w_ofc_inp.append(net.rnn1.weight_ih_l0.detach().numpy())\n",
    "                    w_str_inp.append(net.rnn2.weight_ih_l0.detach().numpy())\n",
    "                except:\n",
    "                    print('issue: something might be up with .model file: '+fname_model)\n",
    "                    #TODO: need to return some nans, becuase this is more of an issue than I thought\n",
    "                    w_ofc_rnn.append(wnan)\n",
    "                    w_str_rnn.append(wnan)\n",
    "                    w_ofc_inp.append(wnan_ofc_inp)\n",
    "                    w_str_inp.append(wnan_str_inp)\n",
    "                \n",
    "\n",
    "\n",
    "                        \n",
    "\n",
    "                    \n",
    "    d = {'w_pred':w_pred ,\n",
    "    'w_mem':w_mem ,\n",
    "    'w_time':w_time,\n",
    "    'w_policy':w_policy ,\n",
    "    'w_val':w_val ,\n",
    "    'w_rr':w_rr,\n",
    "    'w_ofc_rnn':w_ofc_rnn,\n",
    "    'w_str_rnn':w_str_rnn,\n",
    "    'w_ofc_inp':w_ofc_inp,\n",
    "    'w_str_inp':w_str_inp}\n",
    "\n",
    "                \n",
    "    return d, stagelist, idxlist\n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3f631e1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1]\n"
     ]
    }
   ],
   "source": [
    "#check these learning rates\n",
    "lr_task = 0.0001\n",
    "lr_kind = 0.0001\n",
    "lr_pred = 0.005\n",
    "base_epoch_kind = 1000\n",
    "base_epcoh_pred = 20\n",
    "base_epoch_task = 1000\n",
    "\n",
    "r_task = base_epoch_task*lr_task\n",
    "r_kind = base_epoch_kind*lr_kind\n",
    "r_pred = base_epcoh_pred*lr_pred\n",
    "\n",
    "n_int = int(r_task/r_kind) # grab model every 500 saves. did once per epoch\n",
    "n_pred = int(r_task/r_pred)\n",
    "\n",
    "print([n_int, n_pred])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "cd15d333",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10\n",
      "issue: something might be up with .model file: /scratch/dh148/dynamics/results/rnn/ac/20231003/nok_cl/21/rnn_curric_21_block_24.model\n",
      "issue: something might be up with .model file: /scratch/dh148/dynamics/results/rnn/ac/20231003/nok_cl/26/rnn_curric_26_block_24.model\n",
      "issue: something might be up with .model file: /scratch/dh148/dynamics/results/rnn/ac/20231003/nok_cl/27/rnn_curric_27_block_23.model\n",
      "issue: something might be up with .model file: /scratch/dh148/dynamics/results/rnn/ac/20231003/nok_cl/28/rnn_curric_28_block_26.model\n",
      "issue: something might be up with .model file: /scratch/dh148/dynamics/results/rnn/ac/20231003/nok_cl/29/rnn_curric_29_block_5.model\n",
      "issue: something might be up with .model file: /scratch/dh148/dynamics/results/rnn/ac/20231003/nok_cl/29/rnn_curric_29_block_22.model\n",
      "issue: something might be up with .model file: /scratch/dh148/dynamics/results/rnn/ac/20231003/nok_cl/30/rnn_curric_30_block_8.model\n",
      "issue: something might be up with .model file: /scratch/dh148/dynamics/results/rnn/ac/20231003/nok_cl/33/rnn_curric_33_block_28.model\n",
      "issue: something might be up with .model file: /scratch/dh148/dynamics/results/rnn/ac/20231003/nok_cl/34/rnn_curric_34_block_17.model\n",
      "issue: something might be up with .model file: /scratch/dh148/dynamics/results/rnn/ac/20231003/nok_cl/35/rnn_curric_35_block_32.model\n",
      "issue: something might be up with .model file: /scratch/dh148/dynamics/results/rnn/ac/20231003/nok_cl/36/rnn_curric_36_block_19.model\n",
      "issue: something might be up with .model file: /scratch/dh148/dynamics/results/rnn/ac/20231003/nok_cl/37/rnn_curric_37_block_29.model\n",
      "issue: something might be up with .model file: /scratch/dh148/dynamics/results/rnn/ac/20231003/nok_cl/39/rnn_curric_39_block_55.model\n",
      "issue: something might be up with .model file: /scratch/dh148/dynamics/results/rnn/ac/20231003/nok_cl/40/rnn_curric_40_block_29.model\n",
      "issue: something might be up with .model file: /scratch/dh148/dynamics/results/rnn/ac/20231003/nok_cl/43/rnn_curric_43_block_15.model\n",
      "issue: something might be up with .model file: /scratch/dh148/dynamics/results/rnn/ac/20231003/nok_cl/44/rnn_curric_44_block_8.model\n",
      "issue: something might be up with .model file: /scratch/dh148/dynamics/results/rnn/ac/20231003/nok_cl/44/rnn_curric_44_block_52.model\n",
      "issue: something might be up with .model file: /scratch/dh148/dynamics/results/rnn/ac/20231003/nok_cl/45/rnn_curric_45_block_27.model\n",
      "issue: something might be up with .model file: /scratch/dh148/dynamics/results/rnn/ac/20231003/nok_cl/47/rnn_curric_47_block_32.model\n",
      "issue: something might be up with .model file: /scratch/dh148/dynamics/results/rnn/ac/20231003/nok_cl/48/rnn_curric_48_block_13.model\n",
      "issue: something might be up with .model file: /scratch/dh148/dynamics/results/rnn/ac/20231003/nok_cl/49/rnn_curric_49_block_28.model\n",
      "issue: something might be up with .model file: /scratch/dh148/dynamics/results/rnn/ac/20231003/nok_cl/50/rnn_curric_50_block_21.model\n",
      "2\n",
      "20\n",
      "20\r"
     ]
    }
   ],
   "source": [
    "#do over whole population\n",
    "\n",
    "def acrossdat_weightchange(s_idx):\n",
    "    print(s_idx)\n",
    "\n",
    "    lenlist = []\n",
    "    predlist_ofc = []\n",
    "    predlist_str = []\n",
    "    stagelist_list= []\n",
    "    \n",
    "    #max samples for each stage. \n",
    "    nm = [1, 9, 100, 10, 10, 60]\n",
    "    if s_idx == 2:\n",
    "        nn = 20\n",
    "        print(nn)\n",
    "    if s_idx < 2:\n",
    "        nn = 50\n",
    "    if s_idx > 2:\n",
    "        nn = 10\n",
    "    \n",
    "    predmats_ofc = [np.nan*np.ones((nn,k)) for k in nm]\n",
    "    predmats_str = [np.nan*np.ones((nn,k)) for k in nm]\n",
    "    predmats_ofc_inp = [np.nan*np.ones((nn,k)) for k in nm]\n",
    "    predmats_str_inp = [np.nan*np.ones((nn,k)) for k in nm]\n",
    "    \n",
    "    #predmat_str_simple = np.nan*np.ones(nn,nm[0])\n",
    "    #predmat_str_int = np.nan*np.ones(nn,nm[1])\n",
    "    #predmat_str_pred = np.nan*np.ones(nn,nm[2])\n",
    "    #predmat_str_nocatch = np.nan*np.ones(nn,nm[3])\n",
    "    #predmat_str_catch = np.nan*np.ones(nn,nm[4])\n",
    "    #predmat_str_block = np.nan*np.ones(nn,nm[5])\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    for j in range(1,nn+1):\n",
    "        print(j,end='\\r')\n",
    "        nnum = j\n",
    "        d, stagelist, idxlist = weight_outpt_acrosstraining(num = nnum, s_idx = s_idx, n_int=n_int, n_pred=n_pred)\n",
    "          \n",
    "        for k in range(6):\n",
    "            #indices to use for this \n",
    "            idx_use = list(np.argwhere(np.array(stagelist)==k)[:,0])\n",
    "            nidx = len(idx_use)\n",
    "            #idx_use.insert(0,idx_use[0]-1)\n",
    "            \n",
    "            test = np.array([np.linalg.norm(d['w_ofc_rnn'][m]-d['w_ofc_rnn'][m-1]) for m in idx_use]) \n",
    "            predmats_ofc[k][j-1,:len(test)] = test\n",
    "            test = np.array([np.linalg.norm(d['w_str_rnn'][m]-d['w_str_rnn'][m-1]) for m in idx_use]) \n",
    "            predmats_str[k][j-1,:len(test)] = test\n",
    "            \n",
    "            # TODO: add inputs for the supp fig\n",
    "            test = np.array([np.linalg.norm(d['w_ofc_inp'][m]-d['w_ofc_inp'][m-1]) for m in idx_use]) \n",
    "            predmats_ofc_inp[k][j-1,:len(test)] = test\n",
    "            test = np.array([np.linalg.norm(d['w_str_inp'][m]-d['w_str_inp'][m-1]) for m in idx_use]) \n",
    "            predmats_str_inp[k][j-1,:len(test)] = test\n",
    "            \n",
    "        predlist_ofc = np.concatenate(tuple(k for k in predmats_ofc),axis=1)\n",
    "        predlist_str = np.concatenate(tuple(k for k in predmats_str),axis=1)\n",
    "        predlist_ofc_inp = np.concatenate(tuple(k for k in predmats_ofc_inp),axis=1)\n",
    "        predlist_str_inp = np.concatenate(tuple(k for k in predmats_str_inp),axis=1)\n",
    "\n",
    "        \n",
    "    return predlist_ofc, predlist_str, predlist_ofc_inp, predlist_str_inp\n",
    "\n",
    "\n",
    "\n",
    "predlist_ofc_padded, predlist_str_padded, predlist_ofc_inp_padded, predlist_str_inp_padded = acrossdat_weightchange(s_idx = 0)\n",
    "predlist_ofc_padded_nok_cl, predlist_str_padded_nok_cl, predlist_ofc_inp_padded_nok_cl, predlist_str_inp_padded_nok_cl = acrossdat_weightchange(s_idx = 1)\n",
    "predlist_ofc_padded_nok_nocl, predlist_str_inp_padded_nok_nocl, predlist_ofc_inp_padded_nok_nocl, predlist_str_padded_nok_nocl = acrossdat_weightchange(s_idx = 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "0d834ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the lists?\n",
    "\n",
    "savedir = '/scratch/dh148/dynamics/results/rnn/ac/20231003/figs/'\n",
    "\n",
    "with open(savedir+'weights_pop.dat','wb') as f:\n",
    "    pickle.dump([[predlist_ofc_padded,predlist_str_padded,predlist_ofc_inp_padded,predlist_str_inp_padded, ],\n",
    "                 [ predlist_ofc_padded_nok_cl, predlist_str_padded_nok_cl, predlist_ofc_inp_padded_nok_cl,predlist_str_inp_padded_nok_cl],\n",
    "                [ predlist_ofc_padded_nok_nocl,predlist_str_inp_padded_nok_nocl, predlist_ofc_inp_padded_nok_nocl, predlist_str_padded_nok_nocl] ], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e27c568",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchenv",
   "language": "python",
   "name": "torchenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
